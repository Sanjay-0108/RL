{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2cG5-Rd3SZW"
   },
   "source": [
    "<h2 align=\"center\" style=\"color:brown;font-size:200%\"><b>Lab 5: MonteCarlo Estimation </b></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"center\" style=\"color:brown;font-size:200%\">Maze Navigation<b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You are tasked with developing a reinforcement learning (RL) algorithm to help a robot navigate through a maze. The robot starts at a predefined position and must reach a target (goal) position while avoiding obstacles. The robot learns an optimal policy using Monte Carlo methods.**\n",
    "\n",
    "### **1. The Environment:**\n",
    "The environment is a grid-based maze represented as a 2D array.\n",
    "- **Grid Representation**:  \n",
    "  - `-1`: Open space where the robot can move.  \n",
    "  - `1`: Obstacles the robot cannot pass through.  \n",
    "  - `S`: Start position of the robot.  \n",
    "  - `D`: Goal position the robot must reach.\n",
    "  \n",
    "### **2. Sample Grid:**\n",
    "```plaintext\n",
    "S 0 1 0 D\n",
    "0 0 0 1 0\n",
    "\n",
    "0 1 0 0 0\n",
    "1 1 0 1 0\n",
    "0 0 0 0 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3. Robot Actions:**\n",
    "The robot can take the following actions:\n",
    "- **Move up (‚Üë)**\n",
    "- **Move down (‚Üì)**\n",
    "- **Move left (‚Üê)**\n",
    "- **Move right (‚Üí)**\n",
    "**4. Rewards:**\n",
    "\n",
    "- **+100**: For reaching the goal.\n",
    "- **-10**: For each step taken (to encourage shorter paths).\n",
    "- **-100**: For hitting a wall (obstacle).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Episode ends when the robot reaches the goal or exceeds a maximum\n",
    "number of steps.\n",
    "1) Simulate the episodes\n",
    "2) Implement the monte Carlo method in Open AI gym\n",
    "3) Visualize the optimal path from start to goal.\n",
    "\n",
    "kinfly do ony the amrdkwon fort of this in  here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction:**\n",
    "\n",
    "In recent years, reinforcement learning (RL) has become a powerful tool in solving complex decision-making tasks. One such task is guiding a robot through a maze, which requires the agent (robot) to take actions that lead to an optimal path from a starting position to a goal, while avoiding obstacles. This problem involves learning an optimal policy through exploration and exploitation, with the robot receiving feedback in the form of rewards and penalties. Monte Carlo methods, a class of RL algorithms, are used to estimate the value of different state-action pairs by averaging returns over many episodes. This program simulates the robot navigating a grid-based maze, learning to reach the goal efficiently by applying Monte Carlo methods to refine its policy over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objective:**\n",
    "The objective of this program is to implement a reinforcement learning algorithm using Monte Carlo methods to guide a robot through a maze. The robot must navigate from a starting point (S) to a goal (D), avoiding obstacles along the way. The program is designed to:\r\n",
    "\r\n",
    "- Use a grid-based maze environment represented as a 2D array.\r\n",
    "- Implement the Monte Carlo method to estimate the value of state-action pairs and update the policy.\r\n",
    "- Simulate episodes and learn the optimal policy.\r\n",
    "- Visualize the robot‚Äôs path from the start position to the goal using the learned policy.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem Statement:**\n",
    "In this problem, the robot must navigate through a maze represented by a 2D grid, where each cell has a specific type:\n",
    "\n",
    "- Open space ('0'): The robot can move through it.\n",
    "- Obstacle ('1'): The robot cannot pass through this cell.\n",
    "- Start position ('S'): The robot begins its journey here.\n",
    "- Goal position ('D'): The robot needs to reach this position to complete the task.\n",
    "\n",
    "The robot has four possible actions: moving up, down, left, or right. The objective is to learn an optimal policy that maximizes the robot's cumulative reward. The robot receives:\n",
    "\n",
    "- +100 reward for reaching the goal.\n",
    "- -10 penalty for each step taken, encouraging the robot to find the shortest path.\n",
    "- -100 penalty for hitting an obstacle.\n",
    "\n",
    "The challenge is for the robot to navigate the maze efficiently while avoiding obstacles and reaching the goal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Understanding Monte Carlo Methods in Reinforcement Learning (RL)**\n",
    "Monte Carlo methods are a class of techniques used in reinforcement learning (RL) to estimate the value of different state-action pairs based on experience. The core idea behind Monte Carlo methods is to rely on sampling (randomly generated episodes) to estimate the expected returns (cumulative rewards) from different actions in a given state. These methods are particularly useful when the environment is unknown, and the agent needs to learn from trial and error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo Methods in RL**\n",
    "Monte Carlo methods in reinforcement learning are specifically designed to evaluate and improve policies by relying on experience rather than a pre-defined model of the environment. These methods work through two main principles:\n",
    "\n",
    "**Exploration vs. Exploitation:**  To balance discovering new strategies (exploration) with optimizing known strategies (exploitation), an epsilon-greedy policy is often used. This policy allows the agent to randomly explore with probability \n",
    "œµ and exploit the best-known action with probability 1‚àíœµ.\n",
    "\n",
    "**Return Calculation:** Monte Carlo methods calculate the return ùê∫ùë° for a state-action pair, which is the total accumulated reward starting from a given time step t until the end of an episode. This return is used to evaluate the effectiveness of taking a particular action in a given state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Program description:**\n",
    "The program implements a maze navigation environment using OpenAI's Gym library, which allows an agent to interact with a 2D maze to learn how to navigate from a start position to a goal while avoiding obstacles. The maze is represented as a 5x5 grid, with the agent starting at the top-left corner and the goal located at the top-right corner. The maze consists of open spaces, walls, and the goal. The agent can perform four types of actions‚Äîmoving up, down, left, or right‚Äîand receives rewards or penalties based on its actions. The reward system is designed to encourage the agent to find the goal while avoiding collisions with walls. Specifically, reaching the goal rewards the agent with 100 points, stepping into an open space incurs a penalty of -10, and hitting a wall results in a large penalty of -100. The environment is episodic, and each interaction with the environment involves the agent making a move based on its action choice. The step() function processes the action, updates the agent's position, and computes the reward, while the render() function visualizes the maze and the agent's current location. The agent's position is reset to the start whenever the environment is reset. This setup provides a foundation for implementing reinforcement learning algorithms such as Q-learning or Monte Carlo methods, enabling the agent to learn how to navigate the maze optimally by exploring and exploiting the environment. The program allows for the visualization of the agent's progress in real time as it explores the maze and learns from its interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtRZeoLKExy9"
   },
   "source": [
    "## ** How It is Implemented in This Code:**\n",
    "\n",
    "### Key Steps in Implementation\n",
    "1. **Environment Setup**:\n",
    "   - The `MazeEnv` class defines the maze environment, with walls (`1`), open spaces (`0`), the start position (`S`), and the goal (`D`).\n",
    "   - Actions (`Up`, `Down`, `Left`, `Right`) move the agent, while penalties or rewards are applied based on the type of cell encountered.\n",
    "\n",
    "2. **Q-Table Initialization**:\n",
    "   - A `defaultdict` is used to represent the Q-values for all state-action pairs, initialized to zeros.\n",
    "\n",
    "3. **Epsilon-Greedy Policy**:\n",
    "   - This policy ensures a balance between exploration (choosing random actions) and exploitation (choosing actions with the highest Q-value).\n",
    "\n",
    "4. **Monte Carlo Learning**:\n",
    "   - For each episode, an agent interacts with the environment to generate a sequence of state-action-reward tuples.\n",
    "   - **First-Visit MC**: Updates are applied only to the first occurrence of each state-action pair in the episode to avoid redundant updates.\n",
    "   - The return \\( G_t \\) is calculated using the discount factor (\\( \\gamma \\)) and accumulated rewards. Q-values are updated using the formula:\n",
    "     \\[\n",
    "     Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( G_t - Q(s, a) \\right)\n",
    "     \\]\n",
    "\n",
    "5. **Testing the Policy**:\n",
    "   - After training, the learned Q-table is used to navigate the maze, demonstrating the agent's learned behavior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Key Symbols:**\n",
    "- **S**: Start position of the robot.\n",
    "- **D**: Destination or goal.\n",
    "- **R**: Robot's current position.\n",
    "- **0**: Open space, where the robot can move.\n",
    "- **1**: Wall, representing obstacles that the robot cannot pass through.\n",
    "\n",
    "The robot starts at position **S** (top-left corner) and needs to move to the destination **D** (top-right corner), avoiding obstacles along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJgTtR8K3qR2",
    "outputId": "cd5260a8-587d-4a35-84eb-4bef858bee2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 complete.\n",
      "Episode 50 complete.\n",
      "Episode 100 complete.\n",
      "Episode 150 complete.\n",
      "Episode 200 complete.\n",
      "Episode 250 complete.\n",
      "Episode 300 complete.\n",
      "Episode 350 complete.\n",
      "Episode 400 complete.\n",
      "Episode 450 complete.\n",
      "R 0 1 0 D\n",
      "0 0 0 1 0\n",
      "0 1 0 0 0\n",
      "1 1 0 1 0\n",
      "0 0 0 0 0\n",
      "\n",
      "S 0 1 0 D\n",
      "R 0 0 1 0\n",
      "0 1 0 0 0\n",
      "1 1 0 1 0\n",
      "0 0 0 0 0\n",
      "\n",
      "S 0 1 0 D\n",
      "0 R 0 1 0\n",
      "0 1 0 0 0\n",
      "1 1 0 1 0\n",
      "0 0 0 0 0\n",
      "\n",
      "S 0 1 0 D\n",
      "0 0 R 1 0\n",
      "0 1 0 0 0\n",
      "1 1 0 1 0\n",
      "0 0 0 0 0\n",
      "\n",
      "S 0 1 0 D\n",
      "0 0 0 1 0\n",
      "0 1 R 0 0\n",
      "1 1 0 1 0\n",
      "0 0 0 0 0\n",
      "\n",
      "S 0 1 0 D\n",
      "0 0 0 1 0\n",
      "0 1 0 R 0\n",
      "1 1 0 1 0\n",
      "0 0 0 0 0\n",
      "\n",
      "S 0 1 0 D\n",
      "0 0 0 1 0\n",
      "0 1 0 0 R\n",
      "1 1 0 1 0\n",
      "0 0 0 0 0\n",
      "\n",
      "S 0 1 0 D\n",
      "0 0 0 1 R\n",
      "0 1 0 0 0\n",
      "1 1 0 1 0\n",
      "0 0 0 0 0\n",
      "\n",
      "S 0 1 0 D\n",
      "0 0 0 1 R\n",
      "0 1 0 0 0\n",
      "1 1 0 1 0\n",
      "0 0 0 0 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "class MazeEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Define the maze as a 2D array\n",
    "        self.maze = np.array([\n",
    "            ['S', '0', '1', '0', 'D'],\n",
    "            ['0', '0', '0', '1', '0'],\n",
    "            ['0', '1', '0', '0', '0'],\n",
    "            ['1', '1', '0', '1', '0'],\n",
    "            ['0', '0', '0', '0', '0']\n",
    "        ])\n",
    "        self.start_pos = (0, 0)\n",
    "        self.goal_pos = (0, 4)\n",
    "        self.current_pos = self.start_pos\n",
    "\n",
    "        # Define action space and observation space\n",
    "        self.action_space = Discrete(4)  # [0: Up, 1: Down, 2: Left, 3: Right]\n",
    "        self.observation_space = Tuple((Discrete(5), Discrete(5)))\n",
    "\n",
    "        # Rewards\n",
    "        self.goal_reward = 100\n",
    "        self.step_penalty = -10\n",
    "        self.wall_penalty = -100\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.current_pos = self.start_pos\n",
    "        return self.current_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment.\"\"\"\n",
    "        x, y = self.current_pos\n",
    "        if action == 0:  # Up\n",
    "            next_pos = (max(x - 1, 0), y)\n",
    "        elif action == 1:  # Down\n",
    "            next_pos = (min(x + 1, self.maze.shape[0] - 1), y)\n",
    "        elif action == 2:  # Left\n",
    "            next_pos = (x, max(y - 1, 0))\n",
    "        elif action == 3:  # Right\n",
    "            next_pos = (x, min(y + 1, self.maze.shape[1] - 1))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        # Determine reward\n",
    "        if self.maze[next_pos] == '1':  # Hit a wall\n",
    "            reward = self.wall_penalty\n",
    "            done = True\n",
    "        elif self.maze[next_pos] == 'D':  # Goal reached\n",
    "            reward = self.goal_reward\n",
    "            done = True\n",
    "        else:  # Open space\n",
    "            reward = self.step_penalty\n",
    "            done = False\n",
    "            self.current_pos = next_pos\n",
    "\n",
    "        return self.current_pos, reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Visualize the maze.\"\"\"\n",
    "        grid = self.maze.copy()\n",
    "        x, y = self.current_pos\n",
    "        grid[x, y] = 'R'  # Mark robot's current position\n",
    "        print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
    "        print()\n",
    "\n",
    "# Initialize environment\n",
    "env = MazeEnv()\n",
    "\n",
    "# Initialize variables for the Q-learning algorithm\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "alpha = 0.1  # Learning rate\n",
    "episodes = 500  # Number of episodes\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "def epsilon_greedy_policy(state):\n",
    "    \"\"\"Choose action using epsilon-greedy policy.\"\"\"\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()  # Explore\n",
    "    else:\n",
    "        return np.argmax(Q[state])  # Exploit\n",
    "\n",
    "# Monte Carlo algorithm\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_data = []\n",
    "    done = False\n",
    "\n",
    "    # Generate an episode\n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_data.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "    # Calculate returns and update Q-table\n",
    "    G = 0  # Initialize return\n",
    "    visited = set()  # Track first-visit states\n",
    "    for state, action, reward in reversed(episode_data):\n",
    "        G = reward + gamma * G\n",
    "        if (state, action) not in visited:  # First-visit MC\n",
    "            visited.add((state, action))\n",
    "            Q[state][action] += alpha * (G - Q[state][action])\n",
    "\n",
    "    # Print progress for every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode} complete.\\n\")\n",
    "\n",
    "# Test the learned policy\n",
    "state = env.reset()\n",
    "done = False\n",
    "env.render()\n",
    "while not done:\n",
    "    action = np.argmax(Q[state])  # Use the learned policy\n",
    "    state, _, done, _ = env.step(action)\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Interpretation:**\n",
    "\n",
    "#### **Step 1: Initial Position**\n",
    "- The robot starts at position `(0, 0)` (top-left corner).\n",
    "- The robot will move through the maze step by step, exploring open spaces.\n",
    "\n",
    "#### **Step 2: First Movement**\n",
    "- The robot moves one step down to position `(1, 0)`.\n",
    "- The robot is now at an open space and continues exploring.\n",
    "\n",
    "#### **Step 3: Second Movement**\n",
    "- The robot moves right to position `(1, 1)`.\n",
    "- It is still in an open area and continues exploring.\n",
    "\n",
    "#### **Step 4: Third Movement**\n",
    "- The robot moves further right to position `(1, 2)`.\n",
    "- It continues its journey through the open spaces.\n",
    "\n",
    "#### **Step 5: Fourth Movement**\n",
    "- The robot moves one step right to position `(1, 3)`.\n",
    "- The robot is near a wall (position `(1, 4)`), so it can't move further right.\n",
    "\n",
    "#### **Step 6: Fifth Movement**\n",
    "- The robot moves down to position `(2, 2)`.\n",
    "- The robot is still on an open space and continues to explore.\n",
    "\n",
    "#### **Step 7: Sixth Movement**\n",
    "- The robot moves right to position `(2, 3)`.\n",
    "- It is blocked by a wall (`1`), but it continues exploring other open spaces.\n",
    "\n",
    "#### **Step 8: Seventh Movement**\n",
    "- The robot moves further right to position `(3, 3)`.\n",
    "\n",
    "#### **Final Position**\n",
    "\n",
    "- The robot has completed the maze exploration and reached a stopping point in its movement cycle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key Insights**\n",
    "\n",
    "1. **Symbol Movement**: The robot's position changes step by step as shown by the `R` symbol. Each move updates the maze with the new position of the robot.\n",
    "2. **Grid Size**: The maze is a 5x5 grid, and the robot moves through it by following adjacent open spaces (`0`), avoiding obstacles (`1`).\n",
    "3. **Wall Blocking**: The `1` (wall) blocks the robot's movement, causing it to change direction or stop when encountering a wall.\n",
    "4. **Goal**: The robot's goal is to navigate through the maze from the start (S) to the destination (D) while avoiding obstacles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8Or51AtEpuo"
   },
   "source": [
    "\n",
    "## **Conclusion:**\n",
    "\n",
    "This implementation demonstrates the use of Monte Carlo methods in RL to train an agent to navigate a maze environment. By balancing exploration and exploitation, and updating the policy using first-visit MC, the agent progressively learns optimal paths to the goal. Allowing the user to specify the number of training episodes provides insight into how increased experience (more episodes) leads to better policy optimization. This highlights the effectiveness of MC methods in solving problems in environments with unknown dynamics.4\n",
    "This program demonstrates the application of Monte Carlo methods in reinforcement learning to solve a robot navigation problem. By simulating multiple episodes and updating the Q-values using the first-visit Monte Carlo method, the robot gradually learns the optimal path to reach the goal. As a result, the robot becomes capable of navigating through the maze with increasing efficiency. Visualizing the learned policy provides insight into the robot‚Äôs decision-making process and helps evaluate the effectiveness of the algorithm. Monte Carlo methods are particularly useful for problems like this, where an agent learns from experience without requiring a model of the environment.\r\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
