{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb147218-e32e-40f2-b7f1-25d0fa04358d",
   "metadata": {},
   "source": [
    "**<h2 align=\"center\" style=\"color:brown;font-size:200%\">Lab 2: Tic-Tac-Toe Game MDP in OpenAI Gym</h2>**\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2deea3-4acf-47fa-8604-76850686761e",
   "metadata": {},
   "source": [
    "# Modeling Tic-Tac-Toe as a Markov Decision Process (MDP) for Reinforcement Learning\n",
    "\n",
    "## **Introduction:**\n",
    "Tic-Tac-Toe, a simple yet strategic two-player game, provides a compelling case study for reinforcement learning and Markov Decision Processes (MDPs). In this project, we explore how this classic game can be framed as an MDP, allowing an agent to learn optimal strategies through interaction and feedback. By defining the states, actions, rewards, transitions, and policy, we aim to develop a reinforcement learning model capable of understanding and mastering the game dynamics. Furthermore, leveraging OpenAI Gym for implementation enhances the reusability and experimentation potential of our approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e7a8e-1ec1-48c6-b9bc-f21174857988",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "The challenge is to design and implement a reinforcement learning framework for Tic-Tac-Toe, using MDP principles. The goal is to enable an agent to learn the optimal strategy through iterative play, considering the constraints of the game, such as available moves, game outcomes, and rewards. This study also seeks to integrate the model with OpenAI Gym, a popular library for developing and testing reinforcement learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9d4f4-06c9-4b7b-8e6b-e6f3a01ab284",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "1. To define the MDP components for Tic-Tac-Toe, including states, actions, rewards, and transitions.\n",
    "2. To implement a reinforcement learning model capable of learning the optimal policy for playing the game.\n",
    "3. To integrate the model with OpenAI Gym, creating an environment for experimentation.\n",
    "4. To analyze the agent's learning process and evaluate its performance using policy and value functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bdd5439-e9f0-41a1-bbc8-12468ae2ccef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "\n",
      "Step 1:\n",
      "Player X places at position 8\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   | X \n",
      "-----------\n",
      "\n",
      "Reward: 0, Info: {'action': 8}\n",
      "Step 2:\n",
      "Player O places at position 6\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      " O |   | X \n",
      "-----------\n",
      "\n",
      "Reward: 0, Info: {'action': 6}\n",
      "Step 3:\n",
      "Player X places at position 4\n",
      "   |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      " O |   | X \n",
      "-----------\n",
      "\n",
      "Reward: 0, Info: {'action': 4}\n",
      "Step 4:\n",
      "Player O places at position 7\n",
      "   |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      " O | O | X \n",
      "-----------\n",
      "\n",
      "Reward: 0, Info: {'action': 7}\n",
      "Step 5:\n",
      "Player X places at position 1\n",
      "   | X |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      " O | O | X \n",
      "-----------\n",
      "\n",
      "Reward: 0, Info: {'action': 1}\n",
      "Step 6:\n",
      "Player O places at position 0\n",
      " O | X |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      " O | O | X \n",
      "-----------\n",
      "\n",
      "Reward: 0, Info: {'action': 0}\n",
      "Step 7:\n",
      "Player X places at position 2\n",
      " O | X | X \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      " O | O | X \n",
      "-----------\n",
      "\n",
      "Reward: 0, Info: {'action': 2}\n",
      "Step 8:\n",
      "Player X places at position 3\n",
      " O | X | X \n",
      "-----------\n",
      " O | X |   \n",
      "-----------\n",
      " O | O | X \n",
      "-----------\n",
      "\n",
      "Reward: 1, Info: {'result': 'win'}\n",
      "Player O wins!\n",
      "\n",
      "Game Summary:\n",
      "State History:\n",
      "Turn 1: Player X at position 8\n",
      "Turn 2: Player O at position 6\n",
      "Turn 3: Player X at position 4\n",
      "Turn 4: Player O at position 7\n",
      "Turn 5: Player X at position 1\n",
      "Turn 6: Player O at position 0\n",
      "Turn 7: Player X at position 2\n",
      "Turn 8: Player O at position 3\n",
      "\n",
      "Rewards History:\n",
      "[0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gym import Env, spaces\n",
    "\n",
    "\n",
    "class EnhancedTicTacToeEnv(Env):\n",
    "    \"\"\"\n",
    "    Enhanced Tic-Tac-Toe environment with detailed state, action, and reward tracking.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(EnhancedTicTacToeEnv, self).__init__()\n",
    "\n",
    "        # Action space: 9 possible positions (0 to 8)\n",
    "        self.action_space = spaces.Discrete(9)\n",
    "\n",
    "        # Observation space: Board with values (-1: O, 0: empty, 1: X)\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(3, 3), dtype=int)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the game board and other variables for a new game.\n",
    "        \"\"\"\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1  # Player X starts\n",
    "        self.done = False\n",
    "        self.state_history = []  # Track all states\n",
    "        self.action_history = []  # Track all actions\n",
    "        self.reward_history = []  # Track all rewards\n",
    "        return self.board\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute a move and transition to the next state.\n",
    "        \"\"\"\n",
    "        row, col = divmod(action, 3)\n",
    "\n",
    "        # Check for invalid move\n",
    "        if self.board[row, col] != 0:\n",
    "            return self.board, -10, True, {\"invalid_move\": True, \"action\": action}\n",
    "\n",
    "        # Update board\n",
    "        self.board[row, col] = self.current_player\n",
    "        self.action_history.append((self.current_player, action))\n",
    "\n",
    "        # Check for a winner or draw\n",
    "        winner = self._check_winner()\n",
    "        if winner is not None:\n",
    "            self.done = True\n",
    "            if winner == 0:\n",
    "                reward = 0  # Draw\n",
    "                self.reward_history.append(reward)\n",
    "                return self.board, reward, True, {\"result\": \"draw\"}\n",
    "            elif winner == self.current_player:\n",
    "                reward = 1  # Current player wins\n",
    "                self.reward_history.append(reward)\n",
    "                return self.board, reward, True, {\"result\": \"win\"}\n",
    "            else:\n",
    "                reward = -1  # Opponent wins\n",
    "                self.reward_history.append(reward)\n",
    "                return self.board, reward, True, {\"result\": \"lose\"}\n",
    "\n",
    "        # Switch player\n",
    "        self.current_player *= -1\n",
    "\n",
    "        # Game continues\n",
    "        reward = 0  # No reward for non-terminal states\n",
    "        self.reward_history.append(reward)\n",
    "        return self.board, reward, False, {\"action\": action}\n",
    "\n",
    "    def _check_winner(self):\n",
    "        \"\"\"\n",
    "        Check the board for a winner or if the game is a draw.\n",
    "        \"\"\"\n",
    "        for player in [1, -1]:\n",
    "            # Check rows, columns, and diagonals\n",
    "            if any(np.all(self.board == player, axis=0)) or \\\n",
    "               any(np.all(self.board == player, axis=1)) or \\\n",
    "               np.all(np.diag(self.board) == player) or \\\n",
    "               np.all(np.diag(np.fliplr(self.board)) == player):\n",
    "                return player\n",
    "\n",
    "        # Check for draw\n",
    "        if np.all(self.board != 0):\n",
    "            return 0  # Draw\n",
    "\n",
    "        return None  # Game is not finished\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Render the current board state.\n",
    "        \"\"\"\n",
    "        symbols = {0: \" \", 1: \"X\", -1: \"O\"}\n",
    "        for row in self.board:\n",
    "            print(\"|\".join([symbols[cell].center(3) for cell in row]))\n",
    "            print(\"-\" * 11)\n",
    "        print()\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        \"\"\"\n",
    "        Get all valid actions (empty spaces).\n",
    "        \"\"\"\n",
    "        return np.where(self.board.flatten() == 0)[0]\n",
    "\n",
    "\n",
    "# Play a more detailed game\n",
    "def play_detailed_game(env, max_steps=9):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    step_count = 0\n",
    "    while not env.done:\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        if not valid_actions.size:\n",
    "            print(\"No valid moves left. Game ends in a draw.\")\n",
    "            break\n",
    "\n",
    "        # Randomly choose a valid action\n",
    "        action = np.random.choice(valid_actions)\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        step_count += 1\n",
    "        print(f\"Step {step_count}:\")\n",
    "        print(f\"Player {'X' if env.current_player == -1 else 'O'} places at position {action}\")\n",
    "        env.render()\n",
    "        print(f\"Reward: {reward}, Info: {info}\")\n",
    "\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                print(f\"Player {'O' if env.current_player == -1 else 'X'} wins!\")\n",
    "            elif reward == -1:\n",
    "                print(f\"Player {'X' if env.current_player == -1 else 'O'} wins!\")\n",
    "            elif reward == 0:\n",
    "                print(\"It's a draw!\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nGame Summary:\")\n",
    "    print(\"State History:\")\n",
    "    for i, (player, action) in enumerate(env.action_history):\n",
    "        print(f\"Turn {i+1}: Player {'X' if player == 1 else 'O'} at position {action}\")\n",
    "    print(\"\\nRewards History:\")\n",
    "    print(env.reward_history)\n",
    "\n",
    "\n",
    "# Play the enhanced game\n",
    "env = EnhancedTicTacToeEnv()\n",
    "play_detailed_game(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b521d-433f-4ecf-8840-e4b499d83f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11edabfd-396b-42c2-b904-3f8e026b39e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c8d10a-893a-4cfb-83e1-d3ed494e18aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current Board:\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "\n",
      "Player X's turn. Reward: 1. Reason: Move made.\n",
      "\n",
      "Current Board:\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   | X \n",
      "-----------\n",
      "\n",
      "Cumulative Rewards - Player X: 1, Player O: 0\n",
      "Player O's turn. Reward: 1. Reason: Move made.\n",
      "\n",
      "Current Board:\n",
      "   | O |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   | X \n",
      "-----------\n",
      "\n",
      "Cumulative Rewards - Player X: 1, Player O: 1\n",
      "Player X's turn. Reward: 1. Reason: Move made.\n",
      "\n",
      "Current Board:\n",
      "   | O |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      " X |   | X \n",
      "-----------\n",
      "\n",
      "Cumulative Rewards - Player X: 2, Player O: 1\n",
      "Player O's turn. Reward: 4. Reason: Strategic placement towards victory.\n",
      "\n",
      "Current Board:\n",
      " O | O |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      " X |   | X \n",
      "-----------\n",
      "\n",
      "Cumulative Rewards - Player X: 2, Player O: 5\n",
      "Player X's turn. Reward: 6. Reason: Blocked opponent's potential win.\n",
      "\n",
      "Current Board:\n",
      " O | O |   \n",
      "-----------\n",
      " X |   |   \n",
      "-----------\n",
      " X |   | X \n",
      "-----------\n",
      "\n",
      "Cumulative Rewards - Player X: 8, Player O: 5\n",
      "Player O wins!\n",
      "\n",
      "Current Board:\n",
      " O | O | O \n",
      "-----------\n",
      " X |   |   \n",
      "-----------\n",
      " X |   | X \n",
      "-----------\n",
      "\n",
      "Cumulative Rewards - Player X: 8, Player O: 9\n",
      "Game Over!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TicTacToeRewardSystem:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros((3, 3), dtype=int)  # 0 for empty, 1 for X, -1 for O\n",
    "        self.current_player = 1  # 1: X, -1: O\n",
    "        self.done = False\n",
    "        self.x_reward = 0\n",
    "        self.o_reward = 0\n",
    "\n",
    "    def render(self):\n",
    "        symbols = {0: \" \", 1: \"X\", -1: \"O\"}\n",
    "        print(\"\\nCurrent Board:\")\n",
    "        for row in self.state:\n",
    "            print(\"|\".join([symbols[cell].center(3) for cell in row]))\n",
    "            print(\"-\" * 11)\n",
    "        print()\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows, columns, and diagonals for winner\n",
    "        for player in [1, -1]:\n",
    "            if any(np.all(self.state == player, axis=0)) or \\\n",
    "               any(np.all(self.state == player, axis=1)) or \\\n",
    "               np.all(np.diag(self.state) == player) or \\\n",
    "               np.all(np.diag(np.fliplr(self.state)) == player):\n",
    "                return player\n",
    "        if np.all(self.state != 0):  # Draw\n",
    "            return 0\n",
    "        return None  # No winner yet\n",
    "\n",
    "    def step(self, action):\n",
    "        row, col = divmod(action, 3)\n",
    "\n",
    "        if self.state[row, col] != 0:\n",
    "            penalty = -5  # Penalty for invalid moves\n",
    "            if self.current_player == 1:\n",
    "                self.x_reward += penalty\n",
    "            else:\n",
    "                self.o_reward += penalty\n",
    "            return self.state, f\"Invalid move by Player {'X' if self.current_player == 1 else 'O'}. Penalty: {penalty}\", False\n",
    "\n",
    "        # Valid move\n",
    "        self.state[row, col] = self.current_player\n",
    "        reward = 1  # Reward for making a move\n",
    "        reason = \"Move made.\"\n",
    "\n",
    "        # Check adjacency for strategic placement\n",
    "        neighbors = [\n",
    "            (row-1, col), (row+1, col), (row, col-1), (row, col+1),\n",
    "            (row-1, col-1), (row-1, col+1), (row+1, col-1), (row+1, col+1)\n",
    "        ]\n",
    "        adjacency_bonus = any(\n",
    "            0 <= r < 3 and 0 <= c < 3 and self.state[r, c] == self.current_player\n",
    "            for r, c in neighbors\n",
    "        )\n",
    "        if adjacency_bonus:\n",
    "            reward += 3\n",
    "            reason = \"Strategic placement towards victory.\"\n",
    "\n",
    "        # Check for blocking opponent\n",
    "        opponent = -self.current_player\n",
    "        block_bonus = any(\n",
    "            0 <= r < 3 and 0 <= c < 3 and self.state[r, c] == opponent\n",
    "            for r, c in neighbors\n",
    "        )\n",
    "        if block_bonus:\n",
    "            reward += 2\n",
    "            reason = \"Blocked opponent's potential win.\"\n",
    "\n",
    "        if self.current_player == 1:\n",
    "            self.x_reward += reward\n",
    "        else:\n",
    "            self.o_reward += reward\n",
    "\n",
    "        # Check for game end\n",
    "        winner = self.check_winner()\n",
    "        if winner is not None:\n",
    "            self.done = True\n",
    "            if winner == 1:\n",
    "                return self.state, \"Player X wins!\", True\n",
    "            elif winner == -1:\n",
    "                return self.state, \"Player O wins!\", True\n",
    "            else:\n",
    "                return self.state, \"Game ends in a draw.\", True\n",
    "\n",
    "        # Switch player\n",
    "        self.current_player *= -1\n",
    "        return self.state, f\"Player {'X' if self.current_player == -1 else 'O'}'s turn. Reward: {reward}. Reason: {reason}\", False\n",
    "\n",
    "    def cumulative_rewards(self):\n",
    "        return f\"Cumulative Rewards - Player X: {self.x_reward}, Player O: {self.o_reward}\"\n",
    "\n",
    "# Play the game\n",
    "def play_game():\n",
    "    game = TicTacToeRewardSystem()\n",
    "    game.render()\n",
    "\n",
    "    for step in range(9):\n",
    "        valid_moves = [i for i in range(9) if game.state[i // 3, i % 3] == 0]\n",
    "        if not valid_moves:\n",
    "            print(\"No valid moves left. Game ends in a draw.\")\n",
    "            break\n",
    "\n",
    "        action = np.random.choice(valid_moves)\n",
    "        state, message, done = game.step(action)\n",
    "        print(message)\n",
    "        game.render()\n",
    "        print(game.cumulative_rewards())\n",
    "\n",
    "        if done:\n",
    "            print(\"Game Over!\")\n",
    "            break\n",
    "\n",
    "play_game()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd688b-9252-4c22-9e80-d381272ea067",
   "metadata": {},
   "source": [
    "## **Conclusion:**\n",
    "In conclusion, modeling the Tic-Tac-Toe game as a Markov Decision Process (MDP) for reinforcement learning provides a structured approach to solving the game. By breaking the game down into states, actions, rewards, and transitions, we create a dynamic environment where an agent can interact, learn from feedback, and improve its performance. Through iterative learning, the agent can explore various strategies and gradually converge toward an optimal policy that maximizes its chances of winning.\n",
    "\n",
    "MDPs are particularly well-suited to this problem due to their ability to handle decision-making problems where the outcomes depend on both the agent's actions and the random nature of the environment. Reinforcement learning, in turn, allows the agent to adjust its strategy based on the rewards it receives for each action, encouraging faster and more efficient learning.\n",
    "\n",
    "Furthermore, by integrating this model with OpenAI Gym, we enhance the environment's flexibility, allowing the reinforcement learning agent to experiment, learn, and be evaluated in a standardized platform that supports a wide range of RL algorithms. This enables the Tic-Tac-Toe agent to continuously improve its strategy, paving the way for more complex and dynamic learning environments in the future.\n",
    "\n",
    "Ultimately, this project demonstrates the power of MDPs and reinforcement learning in solving structured decision-making problems, such as Tic-Tac-Toe, and highlights their applicability in broader real-world applications, such as robotics, game playing, and automated decision-making systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
