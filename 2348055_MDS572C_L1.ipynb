{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "994e8e90-762c-4746-9c32-fad537da5287",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center;\">LAB EXERCISE1: </div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2770a6-e14a-44f6-94a3-e94dc588de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the states\n",
    "states = ['Far-Red', 'Near-Red', 'At-Red', 'Far-Green', 'Near-Green', 'At-Green', 'Pedestrian', 'Stopped', 'Accident']\n",
    "\n",
    "# Define the actions\n",
    "actions = ['Stop', 'Drive']\n",
    "\n",
    "# Transition matrix - probabilities of moving from one state to another based on action\n",
    "transitions = {\n",
    "    'Far-Red': {'Stop': {'Far-Red': 0.8, 'Near-Red': 0.2}, 'Drive': {'Near-Red': 0.9, 'Pedestrian': 0.1}},\n",
    "    'Near-Red': {'Stop': {'Near-Red': 0.7, 'At-Red': 0.3}, 'Drive': {'At-Red': 0.8, 'Pedestrian': 0.2}},\n",
    "    'At-Red': {'Stop': {'At-Red': 0.9, 'Far-Green': 0.1}, 'Drive': {'Accident': 1.0}},\n",
    "    'Far-Green': {'Drive': {'Near-Green': 1.0}, 'Stop': {'Far-Green': 1.0}},\n",
    "    'Near-Green': {'Drive': {'At-Green': 1.0}, 'Stop': {'Near-Green': 1.0}},\n",
    "    'At-Green': {'Drive': {'Stopped': 1.0}, 'Stop': {'Stopped': 1.0}},\n",
    "    'Pedestrian': {'Stop': {'Pedestrian': 0.8, 'Near-Red': 0.2}, 'Drive': {'Accident': 1.0}},\n",
    "    'Stopped': {'Stop': {'Stopped': 1.0}, 'Drive': {'Accident': 1.0}},\n",
    "    'Accident': {'Stop': {'Accident': 1.0}, 'Drive': {'Accident': 1.0}},\n",
    "}\n",
    "\n",
    "# Rewards for state-action pairs\n",
    "rewards = {\n",
    "    'Far-Red': {'Stop': +1, 'Drive': -5},\n",
    "    'Near-Red': {'Stop': +1, 'Drive': -5},\n",
    "    'At-Red': {'Stop': +1, 'Drive': -20},\n",
    "    'Far-Green': {'Drive': +10, 'Stop': 0},\n",
    "    'Near-Green': {'Drive': +10, 'Stop': 0},\n",
    "    'At-Green': {'Drive': +10, 'Stop': 0},\n",
    "    'Pedestrian': {'Stop': 0, 'Drive': -20},\n",
    "    'Stopped': {'Stop': 0, 'Drive': -20},\n",
    "    'Accident': {'Stop': -20, 'Drive': -20},\n",
    "}\n",
    "\n",
    "# Discount factor (gamma)\n",
    "gamma = 0.9\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "\n",
    "# Function to get the next state\n",
    "def get_next_state(current_state, action):\n",
    "    possible_transitions = transitions[current_state][action]\n",
    "    next_state = random.choices(list(possible_transitions.keys()), list(possible_transitions.values()))[0]\n",
    "    return next_state\n",
    "\n",
    "# Function to update Q-values\n",
    "def q_learning(current_state, action, reward, next_state):\n",
    "    state_index = states.index(current_state)\n",
    "    action_index = actions.index(action)\n",
    "    next_state_index = states.index(next_state)\n",
    "    \n",
    "    # Q-Learning update rule\n",
    "    Q[state_index, action_index] = reward + gamma * np.max(Q[next_state_index])\n",
    "\n",
    "# Simulation loop\n",
    "for episode in range(100):\n",
    "    current_state = 'Far-Red'  # Reset to starting state\n",
    "    for step in range(10):\n",
    "        # Choose action (for simplicity, random choice)\n",
    "        action = random.choice(actions)\n",
    "        \n",
    "        # Get the next state and reward\n",
    "        next_state = get_next_state(current_state, action)\n",
    "        reward = rewards[current_state][action]\n",
    "        \n",
    "        # Update Q-values\n",
    "        q_learning(current_state, action, reward, next_state)\n",
    "        \n",
    "        # Print the state and action\n",
    "        print(f\"Step {step + 1}: State: {current_state}, Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
    "        \n",
    "        # Update the current state\n",
    "        current_state = next_state\n",
    "        \n",
    "        # End the episode if accident occurs\n",
    "        if current_state == 'Accident':\n",
    "            print(\"--- Accident occurred! ---\")\n",
    "            break\n",
    "\n",
    "print(\"\\nFinal Q-table:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb2ed8-379b-4530-be07-2d3aef4ded43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeca469c-2a94-4559-83bc-c399d0453c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Simulation Reset ---\n",
      "Starting State: Far from intersection, Red Light\n",
      "\n",
      "--- Simulation Start ---\n",
      "\n",
      "--- Step 1 ---\n",
      "\n",
      "Action Taken: Stop\n",
      "Reward: +1 for stopping at Red Light.\n",
      "Current State: Near intersection, Red Light\n",
      "\n",
      "--- Step 2 ---\n",
      "\n",
      "Action Taken: Drive\n",
      "Penalty: -10 for running Red Light!\n",
      "Current State: Far from intersection, Red Light\n",
      "\n",
      "--- Step 3 ---\n",
      "\n",
      "Action Taken: Drive\n",
      "Penalty: -10 for running Red Light!\n",
      "Current State: Far from intersection, Red Light\n",
      "\n",
      "--- Step 4 ---\n",
      "\n",
      "Action Taken: Drive\n",
      "Penalty: -10 for running Red Light!\n",
      "Current State: Far from intersection, Red Light\n",
      "\n",
      "--- Step 5 ---\n",
      "\n",
      "Action Taken: Stop\n",
      "Reward: +1 for stopping at Red Light.\n",
      "Current State: Near intersection, Red Light\n",
      "\n",
      "--- Step 6 ---\n",
      "\n",
      "Action Taken: Drive\n",
      "Penalty: -10 for running Red Light!\n",
      "Current State: Far from intersection, Red Light\n",
      "\n",
      "--- Step 7 ---\n",
      "\n",
      "Action Taken: Stop\n",
      "Reward: +1 for stopping at Red Light.\n",
      "Current State: Near intersection, Red Light\n",
      "\n",
      "--- Step 8 ---\n",
      "\n",
      "Action Taken: Drive\n",
      "Penalty: -10 for running Red Light!\n",
      "Current State: Far from intersection, Red Light\n",
      "\n",
      "--- Step 9 ---\n",
      "\n",
      "Action Taken: Stop\n",
      "Reward: +1 for stopping at Red Light.\n",
      "Current State: Near intersection, Red Light\n",
      "\n",
      "--- Step 10 ---\n",
      "\n",
      "Action Taken: Drive\n",
      "Penalty: -10 for running Red Light!\n",
      "Current State: Far from intersection, Red Light\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# Create the custom environment\n",
    "class SelfDrivingCarEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(SelfDrivingCarEnv, self).__init__()\n",
    "        \n",
    "        # Define action and state space\n",
    "        # Actions: 0 = Stop, 1 = Drive\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # States: S1-S5 (Car far/near, red/green light, at intersection)\n",
    "        self.observation_space = spaces.Discrete(5)\n",
    "        \n",
    "        # Rewards\n",
    "        self.reward = 0\n",
    "        \n",
    "        # Initial state\n",
    "        self.state = 0  # Start at far, red light (S1)\n",
    "        \n",
    "        # State descriptions\n",
    "        self.state_desc = {\n",
    "            0: \"Far from intersection, Red Light\",\n",
    "            1: \"Far from intersection, Green Light\",\n",
    "            2: \"Near intersection, Red Light\",\n",
    "            3: \"Near intersection, Green Light\",\n",
    "            4: \"At the intersection\"\n",
    "        }\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Action descriptions\n",
    "        action_desc = {0: \"Stop\", 1: \"Drive\"}\n",
    "\n",
    "        print(f\"\\nAction Taken: {action_desc[action]}\")\n",
    "\n",
    "        # Define rewards and transitions\n",
    "        if self.state == 0:  # S1: far from red light\n",
    "            if action == 0:  # Stop\n",
    "                self.reward = 1  # Positive reward for stopping at red light\n",
    "                print(\"Reward: +1 for stopping at Red Light.\")\n",
    "                self.state = 2  # Transition to near red light\n",
    "            else:  # Drive\n",
    "                self.reward = -10  # Negative reward for running red light\n",
    "                print(\"Penalty: -10 for running Red Light!\")\n",
    "                self.state = 0  # Remain in same state\n",
    "        \n",
    "        elif self.state == 1:  # S2: far from green light\n",
    "            if action == 1:  # Drive\n",
    "                self.reward = 1  # Positive reward for driving at green light\n",
    "                print(\"Reward: +1 for driving through Green Light.\")\n",
    "                self.state = 3  # Move near green light\n",
    "            else:  # Stop\n",
    "                self.reward = -1  # Small penalty for stopping at green light\n",
    "                print(\"Penalty: -1 for stopping unnecessarily at Green Light.\")\n",
    "        \n",
    "        elif self.state == 2:  # S3: near red light\n",
    "            if action == 0:  # Stop\n",
    "                self.reward = 1  # Positive reward for stopping at red light\n",
    "                print(\"Reward: +1 for stopping at Red Light.\")\n",
    "                self.state = 4  # Move to the intersection (S5)\n",
    "            else:  # Drive\n",
    "                self.reward = -10  # Negative reward for running red light\n",
    "                print(\"Penalty: -10 for running Red Light!\")\n",
    "                self.state = 0  # Back to far from intersection, red light\n",
    "        \n",
    "        elif self.state == 3:  # S4: near green light\n",
    "            if action == 1:  # Drive\n",
    "                self.reward = 1  # Positive reward for driving through green light\n",
    "                print(\"Reward: +1 for driving through Green Light.\")\n",
    "                self.state = 4  # Move to the intersection (S5)\n",
    "            else:  # Stop\n",
    "                self.reward = -1  # Small penalty for stopping at green light\n",
    "                print(\"Penalty: -1 for stopping unnecessarily at Green Light.\")\n",
    "        \n",
    "        # Terminal state if the car reaches the intersection\n",
    "        done = bool(self.state == 4)\n",
    "        \n",
    "        # Output the current state in a human-readable format\n",
    "        print(f\"Current State: {self.state_desc[self.state]}\")\n",
    "\n",
    "        # Return observation (state), reward, done, info\n",
    "        return self.state, self.reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset to initial state (far, red light)\n",
    "        self.state = 0\n",
    "        print(\"\\n--- Simulation Reset ---\")\n",
    "        print(\"Starting State: Far from intersection, Red Light\")\n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        # You can add rendering logic here if you want a visual environment\n",
    "        pass\n",
    "\n",
    "# Test the environment\n",
    "env = SelfDrivingCarEnv()\n",
    "state = env.reset()\n",
    "\n",
    "print(\"\\n--- Simulation Start ---\")\n",
    "\n",
    "# Simulate 10 steps in the environment\n",
    "for step in range(10):\n",
    "    print(f\"\\n--- Step {step + 1} ---\")\n",
    "    action = env.action_space.sample()  # Randomly choose an action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        print(\"\\n--- Car has reached the intersection! Simulation End ---\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8522331-5cd2-4bde-ba8f-4405f93fd798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ddb69-7402-449e-89e4-21975c3687e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca00173-c003-47b2-9aba-49453ae3cc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1d6cc-ae6c-4890-ab4d-f77fbdfe5aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110b603-4ff2-474e-88fe-4cbf601db787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526ca8f-bfea-4648-b80a-a39df4f5af0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c03e5-6870-41a4-8193-3fb6a9ab324c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0fe79-6f77-43a5-83b9-627d3a64edd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad556d69-d304-4a63-9476-f951b3b285a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a7f32-0e87-46af-a1b8-acaa18670a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136e180-7e30-4f95-86a4-e06bf82ce877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63644980-4e6b-41bb-aa9f-3def88846dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49506a66-98a6-4ccb-b0c1-1cd4fbc2981a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705c483-21a9-4ff2-b2a9-11659b4f9342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a8d3b-1552-4e96-ab27-6fb7f6f54823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c72cf-113c-4fdb-bd2d-a479b659f7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25514d88-8f96-4203-9f40-cf0332b757b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5784b50c-b775-41fd-90bd-6591f313dddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a341e36f-57e9-4881-8052-4dd005570185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9050f0-9e5b-4caf-92d3-9a74da399a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c813fba-4c86-4ddf-b4d9-3b253ea1e026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
